import faiss
import numpy as np
import os
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

VECTOR_PATH = "data/faiss.index"
CHUNKS_PATH = "data/chunks.pkl"
VECTORIZER_PATH = "data/vectorizer.pkl"

# ğŸ“˜ ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬
with open("data/ìˆ˜í´ìš´ì˜ê·œì¹™.txt", encoding="utf-8") as f:
    raw_text = f.read()

chunks = raw_text.strip().split("\n\n")  # ë¹ˆ ì¤„ ê¸°ì¤€ ë¶„í• 

# ğŸ“Œ ë²¡í„°í™” ë° ì €ì¥ (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±)
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(chunks).toarray().astype(np.float32)

print("ğŸ“¦ ë²¡í„° DBë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
index = faiss.IndexFlatL2(X.shape[1])
index.add(X)
faiss.write_index(index, VECTOR_PATH)

with open(CHUNKS_PATH, "wb") as f:
    pickle.dump(chunks, f)
with open(VECTORIZER_PATH, "wb") as f:
    pickle.dump(vectorizer, f)
print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ")

# ğŸ” ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜
def search_similar_passages(query, top_k=10):
    index = faiss.read_index(VECTOR_PATH)
    with open(CHUNKS_PATH, "rb") as f:
        chunks = pickle.load(f)
    with open(VECTORIZER_PATH, "rb") as f:
        vectorizer = pickle.load(f)

    query_vec = vectorizer.transform([query]).toarray().astype(np.float32)
    D, I = index.search(query_vec, top_k)
    return "\n".join([chunks[i] for i in I[0]])
