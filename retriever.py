import faiss
import numpy as np
import os
import pickle
import re
from sklearn.feature_extraction.text import TfidfVectorizer

VECTOR_PATH = "data/faiss.index"
CHUNKS_PATH = "data/chunks.pkl"
VECTORIZER_PATH = "data/vectorizer.pkl"

# ğŸ“˜ ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬
with open("data/ìˆ˜í´ìš´ì˜ê·œì¹™.txt", encoding="utf-8") as f:
    raw_text = f.read()

chunks = raw_text.strip().split("\n\n")  # ë¹ˆ ì¤„ ê¸°ì¤€ ë¶„í•  ( ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ )

# ğŸ“Œ ë²¡í„°í™” ë° ì €ì¥ (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±)
vectorizer = TfidfVectorizer() # TfidfVectorizerëŠ” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì(ë²¡í„°)ë¡œ ë°”ê¾¸ëŠ” ë„êµ¬
X = vectorizer.fit_transform(chunks).toarray().astype(np.float32)

print("ğŸ“¦ ë²¡í„° DBë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
index = faiss.IndexFlatL2(X.shape[1])
index.add(X)
faiss.write_index(index, VECTOR_PATH) # ë²¡í„° DBì— ì €ì¥

with open(CHUNKS_PATH, "wb") as f:
    pickle.dump(chunks, f)
with open(VECTORIZER_PATH, "wb") as f:
    pickle.dump(vectorizer, f)
print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ")



# âœ… ì§ˆì˜ì—ì„œ ëª¨ë“  ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
def extract_article_numbers(text):
    return re.findall(r'(\d+)ì¡°', text)  # ì˜ˆ: ['1', '2', '9']

# # ğŸ” ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜ (ë‹¤ì¤‘ ì¡°í•­ ëŒ€ì‘)
# ì±—ë´‡ì—ì„œ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ê·¸ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë°”ê¾¸ê³  ë¯¸ë¦¬ ì €ì¥í•´ ë‘” ë²¡í„° DBì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ë‹¨ì„ ê²€ìƒ‰í•´ì„œ LLMì— ë„£ì–´ì„œ ë‹µë³€ ìƒì„±
def search_similar_passages(query, top_k=10):

    index = faiss.read_index(VECTOR_PATH) # ë²¡í„° DB ë¡œë“œ

    with open(CHUNKS_PATH, "rb") as f: # í…ìŠ¤íŠ¸ ì›ë³¸ ë¡œë“œ
        chunks = pickle.load(f)

    with open(VECTORIZER_PATH, "rb") as f: # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë³€í™˜ê¸° ë¡œë“œ ( ì‚¬ìš©ì ì…ë ¥ì„ ë²¡í„°í™” ì‹œí‚¤ëŠ” ì—­í•  )
        vectorizer = pickle.load(f)

    results = []

    # ğŸ“Œ ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ ì§ì ‘ ê²€ìƒ‰
    article_numbers = extract_article_numbers(query)
    for number in article_numbers:
        for chunk in chunks:
            if f"{number}ì¡°" in chunk or f"ì œ{number}ì¡°" in chunk:
                results.append(chunk)
                break  # ì¡°í•­ë³„ 1ê°œë§Œ ì¶”ì¶œ

    # ğŸ” fallback: ì¡°í•­ ë²ˆí˜¸ê°€ ì—†ê±°ë‚˜ ê²€ìƒ‰ì´ ë¶€ì¡±í•œ ê²½ìš° top-k ìœ ì‚¬ ë¬¸ë‹¨ ì¶”ê°€
    if not results:
        query_vec = vectorizer.transform([query]).toarray().astype(np.float32)
        D, I = index.search(query_vec, top_k)
        results = [chunks[i] for i in I[0]]

    return "\n".join(results)
